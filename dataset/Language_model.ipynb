{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 基本概念\n",
    "\n",
    "## 统计语言模型\n",
    "\n",
    "$p(s) = p(w_1) * p(w_2|w_1) * p(w_3|w_0w_1) * …… * p(w_n|w_1w_2……w_{n-1})$\n",
    "\n",
    "说明:\n",
    "+ wi 可以是字、词、短语或词类等等，称为统计基元。通常以“词”代之。\n",
    "+ wi 的概率由 w1, ..., wi-1 决定，由特定的一组w1, ..., wi-1 构成的一个序列，称为 wi 的历史。\n",
    "\n",
    "历史的运用：模型中有 $L^{i-1}$ 个参数 $p(wi|w_1...w_{i-1})$\n",
    "\n",
    "其中 $L$ 表示共有 $L$ 个不同的基元（如有 $L$个不同的词）\n",
    "\n",
    "指数型增长 $\\rightarrow$ 爆炸\n",
    "\n",
    "解决方案：只考虑固定长度的滑动窗口，即在长度为 $n$ 内若 $(w_{i-n+1},……,w_{i-1}) = (v_{k-n+1},……,v_{k-1})$ ，则认为，若 $w_{i} = v_{k}$，则 $p(w_i|S(w_{i-n+1},……,w_{i-1})) = p(v_k|S(v_{k-n+1},……,v_{k-1}))$"
   ],
   "id": "a054e0f5a184ff73"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# n 元文法模型（n-gram）\n",
    "\n",
    "**给定句子**：John read a book\n",
    "\n",
    "**增加标记**：\\<BOS\\> John read a book\\<EOS\\>\n",
    "\n",
    "**Unigram(1-gram)**: \\<BOS\\>, John, read, a, book, \\<EOS\\>\n",
    "\n",
    "**Bigram(2-gram)**:(\\<BOS\\>John), (John read), (read a),(a book), (book \\<EOS\\>)\n",
    "\n",
    "**Trigram(3-gram)**:(\\<BOS\\>John read), (John read a),(read a book), (a book \\<EOS\\>)\n",
    "\n",
    "**定义**：$p(s) = \\prod_i^{m+1} p(w_i|w_{i-n+1},……,w_{i-1})$\n",
    "\n",
    "$w_0$ 为 \\<BOS\\>，$w_{m+1}$ 为 \\<EOS\\>。\n",
    "\n",
    "**实现代码：**"
   ],
   "id": "88d1771c949887b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T03:05:19.591194Z",
     "start_time": "2025-12-26T03:05:19.577581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sympy import Segment\n",
    "\n",
    "\n",
    "def create_ngram_model(text,n):\n",
    "    words = text.split()\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram = words[i: i + n]\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "# main:\n",
    "n = 3\n",
    "text = \"<BOS> John read a book <EOS>\"\n",
    "print(create_ngram_model(text,n))"
   ],
   "id": "eaf53b8017cbce76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<BOS>', 'John', 'read'], ['John', 'read', 'a'], ['read', 'a', 'book'], ['a', 'book', '<EOS>']]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 应用示例\n",
    "\n",
    "### 1.音字转换问题\n",
    "\n",
    "输入拼音串，输出可能的句子\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{CString} &= \\arg\\max\\limits_{CString} p(CString \\mid Pinyin) \\\\\n",
    "&= \\arg\\max\\limits_{CString} \\frac{p(Pinyin \\mid CString) \\times p(CString)}{p(Pingyin)} \\\\\n",
    "&= \\arg\\max\\limits_{CString} p(Pinyin \\mid CString) \\times p(CString) \\\\\n",
    "&= \\arg\\max\\limits_{CString} p(CString)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**解释：** 前面是条件概率基本应用，化简时 Pingyin 的消除其实是因为：**只要找最合适的，不需要真的算出概率**\n",
    "\n",
    "(1)当前给的 Pingyin 已经固定，我们只关注谁的概率更高，因此相当于比较分子大小\n",
    "\n",
    "(2)$p(Pingyin|Cstring) = 1$ ，这是显然的，Cstring 固定了就可以唯一确定 Pingyin，而计算时我们只找Pingyin符合的Cstring（实际上并非恒等变化，但还是同理，我们只想找出最合适的，那限定范围就在Pingyin符合的）\n",
    "\n",
    "**结论：** 等于在语料库中出现的概率"
   ],
   "id": "a0e1b759bee6cf4a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.汉语分词问题\n",
    "\n",
    "给定汉字串：他是研究生物的，\n",
    "可能的汉字串：\n",
    "1）他|是|研究生|物|的\n",
    "2）他|是|研究|生物|的\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{Seg} &= \\arg\\max\\limits_{Seg} p(Seg \\mid Text) \\\\\n",
    "&= \\arg\\max\\limits_{Seg} \\frac{p(Text \\mid Seg) \\times p(Seg)}{p(Text)} \\\\\n",
    "&= \\arg\\max\\limits_{Seg} p(Text \\mid Seg) \\times p(Seg) \\\\\n",
    "&= \\arg\\max\\limits_{Seg} p(Seg)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "式中 $Text$ 是一个句子，$Seg$ 代表对 $Text$ 的划分"
   ],
   "id": "c20fa3cccf17c3b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**实战：** 下面准备了相关的示例，先导入数据集，再运行计算代码",
   "id": "5b3a4517bb67385f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T09:51:09.113272Z",
     "start_time": "2025-12-27T09:51:09.106409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 汉语分词问题\n",
    "\n",
    "dataset_train_Chinese = [\n",
    "    # === 场景一：地理与地标（用于强化“南京市”和“长江大桥”） ===\n",
    "    \"南京市 是 江苏省 的 省会\",\n",
    "    \"宏伟 的 长江大桥 横跨 江面\",\n",
    "    \"去 南京市 长江大桥 看 风景\",\n",
    "    \"车辆 行驶 在 长江大桥 上\",\n",
    "\n",
    "    # === 场景二：职场与人物（用于制造“市长”和“江大桥”的歧义） ===\n",
    "    # 设定：假设有一个叫“江大桥”的虚构人物，经常和“南京市长”一起出现\n",
    "    \"南京 市长 江大桥 发表 了 演讲\",\n",
    "    \"江大桥 是 一位 勤奋 的 市长\",\n",
    "    \"欢迎 南京 市长 来 参观\",\n",
    "]\n",
    "dataset_test_divideChinese = [\n",
    "    # 经典歧义：模型是切出 \"长江大桥\"(地标) 还是 \"江大桥\"(人名)？\n",
    "    # 取决于你在训练集中喂了更多关于“桥梁”的数据，还是“市长”的数据\n",
    "    \"南京市长江大桥\",\n",
    "\n",
    "    # 边界测试\n",
    "    \"南京市长\"\n",
    "]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_ngram_model(text:str,n):\n",
    "    words = text.split()\n",
    "    words.insert(0,\"<BOS>\")\n",
    "    words.append(\"<EOS>\")\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram = words[i: i + n]\n",
    "        ngrams.append(ngram)\n",
    "    return words,ngrams\n",
    "\n",
    "def train_ngrams(n, dataset):\n",
    "    # 若键值不存在，默认次数为0\n",
    "    model = defaultdict(int)\n",
    "    vocabulary = set()\n",
    "    for text in dataset:\n",
    "        words,ngrams = create_ngram_model(text,n)\n",
    "        for word in words:\n",
    "            vocabulary.add(word)\n",
    "        for ngram in ngrams:\n",
    "            # 字典的键值必须不可变，这里转化成元组\n",
    "            # 1. ngram出现的次数\n",
    "            ngram_tuple = tuple(ngram)\n",
    "            model[ngram_tuple] += 1\n",
    "\n",
    "            # 2. 前置n-1出现的次数\n",
    "            context_tuple = ngram_tuple[:-1]\n",
    "            model[context_tuple] += 1\n",
    "    return vocabulary,model\n",
    "\n",
    "def getSegments(maxlen,text:str,segment:list[str]):\n",
    "    global segmentSet\n",
    "    if len(text) == 0:\n",
    "        segment.insert(0,\"<BOS>\")\n",
    "        segment.append(\"<EOS>\")\n",
    "        segmentSet.append(segment)\n",
    "        return\n",
    "    for i in range(min(len(text),maxlen)):\n",
    "        getSegments(maxlen,text[(i+1):],segment + [text[:(i+1)]])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n = 2   # n元文法模型：n-gram\n",
    "    m = 4   # 单词长度切分\n",
    "\n",
    "    vocabulary,model = train_ngrams(n,dataset_train_Chinese)\n",
    "\n",
    "    print(\"solution begin!\")\n",
    "\n",
    "    for text in dataset_test_divideChinese:\n",
    "        # 初始化\n",
    "        p = 0.0\n",
    "        ans = [\"Error\"]\n",
    "        segmentSet = []\n",
    "\n",
    "        # 获取所有可能切分\n",
    "        getSegments(m,text,[])\n",
    "\n",
    "        for segment in segmentSet:\n",
    "            p_temp = 1.0\n",
    "            for i in range(0,len(segment) - n + 1):\n",
    "                t = tuple(segment[i:i + n])\n",
    "                count_all = model[t[:-1]]\n",
    "                count_now = model[t]\n",
    "                if count_all != 0:\n",
    "                    # 做加一平滑\n",
    "                    p_temp *= (count_now + 1.0) / (count_all + len(vocabulary))\n",
    "                    # 可以替换如果不做平滑处理的后果\n",
    "                    # p_temp *= count_now / count_all\n",
    "                else:\n",
    "                    p_temp *= 0\n",
    "\n",
    "            # 最优化记录\n",
    "            if p_temp > p:\n",
    "                p = p_temp\n",
    "                ans = segment\n",
    "\n",
    "        print(f\"I think {text} is {ans[1:-1]} (P = {p})\")"
   ],
   "id": "d3d540a6a844b753",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution begin!\n",
      "I think 南京市长江大桥 is ['南京市', '长江大桥'] (P = 0.00011200716845878135)\n",
      "I think 南京市长 is ['南京', '市长'] (P = 0.00033602150537634406)\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T10:44:56.939556Z",
     "start_time": "2025-12-27T10:44:56.927596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 拼音转汉语问题\n",
    "# 顺序的调换是想先做切分，再映射为汉语\n",
    "# 学习的时候不仅生成n-gram，同时做拼音到汉语的对应dict\n",
    "# 思路大体一致，vibe coding：\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# === 1. 数据集 (汉字已分词, 拼音一一对应) ===\n",
    "dataset_train = [\n",
    "    # ==========================================\n",
    "    # 场景一：“研究” (Verb) vs “烟酒” (Noun)\n",
    "    # 目的：训练 P(研究|做/搞) 高， P(烟酒|喜欢/买) 高\n",
    "    # ==========================================\n",
    "    (\"他 喜欢 烟酒\", \"ta xi huan yan jiu\"),\n",
    "    (\"烟酒 不 分家\", \"yan jiu bu fen jia\"),\n",
    "    (\"商店 卖 烟酒\", \"shang dian mai yan jiu\"),\n",
    "    (\"远离 烟酒 危害\", \"yuan li yan jiu wei hai\"),\n",
    "    (\"过度 沉迷 烟酒\", \"guo du chen mi yan jiu\"),\n",
    "\n",
    "    (\"正在 做 研究\", \"zheng zai zuo yan jiu\"),\n",
    "    (\"科学 研究 很 重要\", \"ke xue yan jiu hen zhong yao\"),\n",
    "    (\"专心 搞 研究\", \"zhuan xin gao yan jiu\"),\n",
    "    (\"研究 成果 显著\", \"yan jiu cheng guo xian zhu\"),\n",
    "    (\"他 在 研究所 工作\", \"ta zai yan jiu suo gong zuo\"), # 引入\"研究所\"词汇\n",
    "\n",
    "    # ==========================================\n",
    "    # 场景二：“研究生” (Noun) vs “研究 生物” (Verb + Noun)\n",
    "    # 目的：这是最难的歧义。\n",
    "    # 如果后面跟的是“毕业/考试”，应该切成“研究生”。\n",
    "    # 如果后面跟的是“的/特性”，且前面是动词语境，可能切成“研究 生物”。\n",
    "    # ==========================================\n",
    "    # 强化 \"研究生\" (作为一个整体词)\n",
    "    (\"他 是 研究生\", \"ta shi yan jiu sheng\"),\n",
    "    (\"研究生 毕业 了\", \"yan jiu sheng bi ye le\"),\n",
    "    (\"报考 研究生\", \"bao kao yan jiu sheng\"),\n",
    "    (\"学历 是 研究生\", \"xue li shi yan jiu sheng\"),\n",
    "    (\"读 研究生 很 累\", \"du yan jiu sheng hen lei\"),\n",
    "\n",
    "    # 强化 \"研究\" + \"生物\" (Bigram)\n",
    "    (\"研究 生物 的 构造\", \"yan jiu sheng wu de gou zao\"),\n",
    "    (\"他 想 研究 生物\", \"ta xiang yan jiu sheng wu\"),\n",
    "    (\"我们 研究 生物 钟\", \"wo men yan jiu sheng wu zhong\"),\n",
    "    (\"生物 是 一门 学科\", \"sheng wu shi yi men xue ke\"), # 强化“生物”作为独立词的频率\n",
    "    (\"海洋 生物 很多\", \"hai yang sheng wu hen duo\"),\n",
    "\n",
    "    # ==========================================\n",
    "    # 场景三：“他是” (Subj+Verb) vs “踏实” (Adj)\n",
    "    # 目的：区分 ta shi\n",
    "    # ==========================================\n",
    "    (\"做事 要 踏实\", \"zuo shi yao ta shi\"),\n",
    "    (\"踏实 肯干\", \"ta shi ken gan\"),\n",
    "    (\"睡 得 很 踏实\", \"shui de hen ta shi\"),\n",
    "    (\"做人 要 踏实\", \"zuo ren yao ta shi\"),\n",
    "\n",
    "    (\"他 是 老师\", \"ta shi lao shi\"),\n",
    "    (\"他 是 科学家\", \"ta shi ke xue jia\"),\n",
    "    (\"她 是 医生\", \"ta shi yi sheng\"), # 注意：拼音ta一样，汉字不同，增加候选词\n",
    "    (\"它 是 机器\", \"ta shi ji qi\"),\n",
    "\n",
    "    # ==========================================\n",
    "    # 场景四：词汇覆盖 (防止出现 Unknown 字导致概率断裂)\n",
    "    # ==========================================\n",
    "    (\"无 忧 无 虑\", \"wu you wu lv\"), # 覆盖 \"wu\" -> 无\n",
    "    (\"前面 有 大雾\", \"qian mian you da wu\"), # 覆盖 \"wu\" -> 雾\n",
    "    (\"不仅 如此\", \"bu jin ru ci\"),\n",
    "    (\"的\", \"de\"), # 这是一个Trick，保证常用虚词在词典里\n",
    "    (\"了\", \"le\")\n",
    "]\n",
    "\n",
    "# === 2. 自动训练：同时学习“词典”和“概率” ===\n",
    "def train_model_from_corpus(dataset):\n",
    "    # 词典：Key=拼音串, Value=可能的汉字词集合\n",
    "    # e.g. \"yan jiu\" -> {\"研究\", \"烟酒\"}\n",
    "    pinyin_to_word = defaultdict(set)\n",
    "\n",
    "    # 概率统计\n",
    "    word_bigram_counts = defaultdict(int)\n",
    "    word_unigram_counts = defaultdict(int)\n",
    "    vocab_size = 0\n",
    "\n",
    "    for hanzi_str, pinyin_str in dataset:\n",
    "        h_words = hanzi_str.split()  # [\"他\", \"是\", \"研究生\"]\n",
    "        p_tokens = pinyin_str.split() # [\"ta\", \"shi\", \"yan\", \"jiu\", \"sheng\"]\n",
    "\n",
    "        # --- 核心逻辑：自动对齐提取词典 ---\n",
    "        p_cursor = 0 # 拼音游标\n",
    "\n",
    "        # 加上 BOS/EOS 用于语言模型训练\n",
    "        training_words = [\"<BOS>\"] + h_words + [\"<EOS>\"]\n",
    "\n",
    "        # 1. 遍历汉字词，切取对应的拼音\n",
    "        for word in h_words:\n",
    "            # 获取当前词的字数，比如 \"研究生\" 长度为 3\n",
    "            word_len = len(word)\n",
    "\n",
    "            # 从拼音列表中切出对应长度的片段\n",
    "            # 比如 cursor=2, len=3, 切出 p_tokens[2:5] -> [\"yan\", \"jiu\", \"sheng\"]\n",
    "            current_pinyin_segment = p_tokens[p_cursor : p_cursor + word_len]\n",
    "\n",
    "            # 拼合成字符串作为 Key\n",
    "            pinyin_key = \" \".join(current_pinyin_segment)\n",
    "\n",
    "            # 存入词典：学会了这个拼音串对应这个词\n",
    "            pinyin_to_word[pinyin_key].add(word)\n",
    "\n",
    "            # 移动游标\n",
    "            p_cursor += word_len\n",
    "\n",
    "        # 2. 统计 N-gram 概率 (和之前一样)\n",
    "        for i in range(len(training_words)-1):\n",
    "            w_curr = training_words[i]\n",
    "            w_next = training_words[i+1]\n",
    "            word_unigram_counts[w_curr] += 1\n",
    "            word_bigram_counts[(w_curr, w_next)] += 1\n",
    "\n",
    "        vocab_size += len(training_words)\n",
    "\n",
    "    return pinyin_to_word, word_bigram_counts, word_unigram_counts, vocab_size\n",
    "\n",
    "# === 3. 拼音切分与解码 (Lattice 生成) ===\n",
    "# 这里逻辑不变：尝试切分输入拼音，看词典里有没有\n",
    "def solve_pinyin_lattice(pinyin_list, current_words, results, dictionary):\n",
    "    if not pinyin_list:\n",
    "        results.append(current_words)\n",
    "        return\n",
    "\n",
    "    max_search_len = 4 # 假设最长词只有4个字，防死循环\n",
    "\n",
    "    # 尝试切出前 k 个拼音\n",
    "    for k in range(1, min(len(pinyin_list) + 1, max_search_len + 1)):\n",
    "        segment = pinyin_list[:k]\n",
    "        segment_key = \" \".join(segment)\n",
    "\n",
    "        # 如果这个拼音串在刚才自动学习的词典里存在\n",
    "        if segment_key in dictionary:\n",
    "            possible_words = dictionary[segment_key]\n",
    "            for word in possible_words:\n",
    "                # 递归继续切分剩余部分\n",
    "                solve_pinyin_lattice(pinyin_list[k:], current_words + [word], results, dictionary)\n",
    "\n",
    "# === 4. 计算句子概率 ===\n",
    "def get_score(sentence, bi_counts, uni_counts, v_size):\n",
    "    prob = 1.0\n",
    "    full = [\"<BOS>\"] + sentence + [\"<EOS>\"]\n",
    "    for i in range(len(full)-1):\n",
    "        pre = full[i]\n",
    "        cur = full[i+1]\n",
    "        # 加一平滑\n",
    "        prob *= (bi_counts[(pre, cur)] + 1) / (uni_counts[pre] + v_size)\n",
    "    return prob\n",
    "\n",
    "# === 主程序 ===\n",
    "if __name__ == '__main__':\n",
    "    # 1. 自动从数据中学习\n",
    "    print(\"正在从语料库提取词典并训练模型...\")\n",
    "    p2w_dict, bi_counts, uni_counts, v_size = train_model_from_corpus(dataset_train)\n",
    "\n",
    "    # 打印一下自动学到的词典看看\n",
    "    print(\"--- 自动提取的词典 (部分) ---\")\n",
    "    for py, words in list(p2w_dict.items())[:5]:\n",
    "        print(f\"[{py}] -> {words}\")\n",
    "\n",
    "    # 2. 测试输入\n",
    "    # 这是一个很有趣的测试：它既可以是 \"研究 生物\", 也可以是 \"研究生 (wu?)\"\n",
    "    # 但因为我们只学过 \"wu\"->\"无\"，也学过 \"sheng wu\"->\"生物\"\n",
    "    pinyinset = [\"yan jiu sheng wu\",\"ta xi huan yan jiu\",\"ta zai yan jiu sheng wu\" ,\"yan jiu sheng bi ye le\"]\n",
    "    for target_pinyin in pinyinset:\n",
    "        print(f\"--- 开始解码: {target_pinyin} ---\")\n",
    "        candidates = []\n",
    "        solve_pinyin_lattice(target_pinyin.split(), [], candidates, p2w_dict)\n",
    "\n",
    "        # 3. 寻找最优解\n",
    "        best_sent = []\n",
    "        best_score = -1\n",
    "\n",
    "        for sent in candidates:\n",
    "            score = get_score(sent, bi_counts, uni_counts, v_size)\n",
    "            sent_str = \" \".join(sent)\n",
    "            print(f\"候选: {sent_str} \\t(P={score:.6e})\")\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_sent = sent_str\n",
    "\n",
    "        print(f\"✅ 最终识别结果:【{best_sent}】\")"
   ],
   "id": "5f471ab68bee3db6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从语料库提取词典并训练模型...\n",
      "--- 自动提取的词典 (部分) ---\n",
      "[ta] -> {'她', '它', '他'}\n",
      "[xi huan] -> {'喜欢'}\n",
      "[yan jiu] -> {'烟酒', '研究'}\n",
      "[bu] -> {'不'}\n",
      "[fen jia] -> {'分家'}\n",
      "--- 开始解码: yan jiu sheng wu ---\n",
      "候选: 烟酒 生物 \t(P=6.760411e-07)\n",
      "候选: 研究 生物 \t(P=4.009623e-06)\n",
      "候选: 研究生 无 \t(P=3.440209e-07)\n",
      "✅ 最终识别结果:【研究 生物】\n",
      "--- 开始解码: ta xi huan yan jiu ---\n",
      "候选: 她 喜欢 烟酒 \t(P=1.647946e-08)\n",
      "候选: 她 喜欢 研究 \t(P=6.108765e-09)\n",
      "候选: 它 喜欢 烟酒 \t(P=1.647946e-08)\n",
      "候选: 它 喜欢 研究 \t(P=6.108765e-09)\n",
      "候选: 他 喜欢 烟酒 \t(P=1.120222e-07)\n",
      "候选: 他 喜欢 研究 \t(P=4.152548e-08)\n",
      "✅ 最终识别结果:【他 喜欢 烟酒】\n",
      "--- 开始解码: ta zai yan jiu sheng wu ---\n",
      "候选: 她 在 烟酒 生物 \t(P=2.395270e-11)\n",
      "候选: 她 在 研究 生物 \t(P=9.470954e-11)\n",
      "候选: 她 在 研究生 无 \t(P=1.218895e-11)\n",
      "候选: 它 在 烟酒 生物 \t(P=2.395270e-11)\n",
      "候选: 它 在 研究 生物 \t(P=9.470954e-11)\n",
      "候选: 它 在 研究生 无 \t(P=1.218895e-11)\n",
      "候选: 他 在 烟酒 生物 \t(P=1.628230e-10)\n",
      "候选: 他 在 研究 生物 \t(P=6.438059e-10)\n",
      "候选: 他 在 研究生 无 \t(P=8.285668e-11)\n",
      "✅ 最终识别结果:【他 在 研究 生物】\n",
      "--- 开始解码: yan jiu sheng bi ye le ---\n",
      "候选: 研究生 毕业 了 \t(P=2.457292e-08)\n",
      "✅ 最终识别结果:【研究生 毕业 了】\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## n 元语法模型的获取\n",
    "\n",
    "**训练语料：** 用于建立模型，确定模型参数的己知语料\n",
    "\n",
    "**最大似然估计：** 可以得出，用相对频率计算概率的方法，概率最大\n",
    "\n",
    "$$p(w_i \\mid w_{i-n+1}^{i-1}) = f(w_i \\mid w_{i-n+1}^{i-1}) = \\frac{c(w_{i-n+1}^i)}{\\sum_{w_i} c(w_{i-n+1}^i)}$$\n",
    "\n",
    "其中 $c(w^j_i)$ 表示语料库中 $k$ 从 $i$ 到 $j$ 分别为 $w_k$ 的情况的计数，$\\sum_{w_i} c(w_{i-n+1}^i)$ 实际上是前 $n-1$ 符合的情况下，最后一个可能的所有情况计数"
   ],
   "id": "523c5bfe1684b189"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 数据平滑\n",
    "\n",
    "**基本思想：** 劫富济贫，0 和 1 差太多，减少正确和错误之前的差异\n",
    "\n",
    "**基本约束：**\n",
    "\n",
    "$$\n",
    "\\sum_{w_i} p(w_i \\mid w_1, w_2, \\cdots, w_{i-1}) = 1\n",
    "$$\n",
    "\n",
    "**方法：**\n",
    "\n",
    "加 1 法： 对所有情况出现次数 + 1\n",
    "\n",
    "\\begin{align*}\n",
    "p(w_i \\mid w_{i-n+1}…… w_{i-1}) &= \\frac{1 + c(w_{i-n+1}……w_{i-1}w_i)}{\\sum_{w_i} \\left[1 + c(w_{i-n+1}……w_{i-1}w_i)\\right]} \\\\\n",
    "                    &= \\frac{1 + c(w_{i-n+1}……w_{i-1}w_i)}{|V| + \\sum_{w_i} c(w_{i-n+1}……w_{i-1}w_i)}\n",
    "\\end{align*}\n",
    "\n",
    "其中 $|V|$ 是所有可能的的词汇量（其实此处并非严格等于，但是实际应用中差别不大）\n",
    "\n",
    "<mark>\\<BOS\\> \\<EOS\\> 不需要计入 $|V|$ </mark>，但这到底是为什么呢？"
   ],
   "id": "754d5be325c986c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# n元模型应用\n",
    "\n",
    "## 采用基于语言模型的分词方法\n",
    "\n",
    "这一部分笔者觉得相当抽象，个人（~~AI~~）理解要解决的问题是：\n",
    "很多词（如人名、地名、具体的时间“3月14日”）在训练语料里根本没出现过，概率为0。如果把所有可能的数字、人名都存进词表，词表会无限大。\n",
    "\n",
    "因此用一些标签来代替，定义了四类：\n",
    "+ **分词词典中规定的词**;\n",
    "+ **由词法规则派生出来的词或短语**，如:干干净净、非党员、全面性、检查员、看不出、克服了、走出来...\n",
    "+ **与数字相关的实体**，如:日期、时间、货币、百分数、温度、长度、面积、重量、电话号码、邮件地址等;\n",
    "+ **专有名词**，如:人名（PN）、地名（LP）、机构名（ON）。\n",
    "\n",
    "**规定：** PN，LP，ON 分别单独为一类，实体FT（包括dat、tim、per、mon等等）作为一个类\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{C} &= \\arg\\max\\limits_{C} p(C \\mid S) \\\\\n",
    "&= \\arg\\max\\limits_{C} \\frac{p(C) \\times p(S \\mid C)}{p(S)} \\\\\n",
    "&\\stackrel{归一化}= \\arg\\max\\limits_{C} p(C) \\times p(S \\mid C)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**语言模型** $p(C)$ 可采用三元语法，其实就是把之前的 $Seg、Word$ 换成 $Class$\n",
    "\n",
    "**生成模型** $p(S\\mid C) \\approx \\prod_{i=1}^N p(s_i|c_i)$\n",
    "\n",
    "**含义：** 已知是哪一类，生成对应汉字串 s 的概率\n",
    "\n",
    "计算如下：\n",
    "\n",
    "1. 若 C 为 FT、LW（Lexicon Word词表词）、MW（Morphological Word词法派生词）查对应的表，有则 $P(s|c) = 1$，否则为 $0$\n",
    "2. PN、LN 均为基于字的 2 元模型\n",
    "3. ON 为基于词的 2 元模型\n",
    "\n",
    "所谓基于字的 2 元模型是一个子模型。\n",
    "\n",
    "比如计算“比尔盖茨”是人名的概率，要看“比-尔”、“尔-盖”、“盖-茨”这些字在人名库中相邻出现的概率。\n",
    "\n",
    "**训练算法**\n",
    "\n",
    "这是一个“鸡生蛋，蛋生鸡”的问题：只有分好词才能训练模型，但只有有了模型才能分好词。\n",
    "\n",
    "+ 初始化：先用基础分词器切一版粗糙的语料。\n",
    "+ M步 (Maximization)：用这一版语料统计参数，算出公式(3)中的概率。\n",
    "+ E步 (Expectation)：用算出的新模型，重新对语料进行更准确的切分。\n",
    "+ 循环：重复2-3步，直到结果稳定。\n",
    "\n",
    "**分词和词性判断一体化：**\n",
    "\n",
    "核心思想：二者信息可以相互辅助\n",
    "\n",
    "**子模型 1：基于词性的生成模型**\n",
    "$$ P(W, T) \\approx \\prod P(w_i \\mid t_i) \\times P(t_i \\mid t_{i-1}, t_{i-2}) $$\n",
    "*   **物理意义**：这就是标准的HMM（隐马尔可夫模型）。\n",
    "    *   $P(t_i \\mid t_{i-1}, t_{i-2})$：词性转移概率（如：名词后面接动词的概率）。\n",
    "    *   $P(w_i \\mid t_i)$：发射概率（如：已知是动词，这个词是“写”的概率）。\n",
    "\n",
    "**子模型 2：基于单词的统计模型**\n",
    "$$ P(W, T) \\approx \\prod P(t_i \\mid w_i) \\times P(w_i \\mid w_{i-1}, w_{i-2}) $$\n",
    "*   **物理意义**：\n",
    "    *   $P(w_i \\mid w_{i-1}, w_{i-2})$：标准的词汇3-gram（如：“文章”后面接“写”的概率）。\n",
    "    *   $P(t_i \\mid w_i)$：这个词本身具有某个词性的概率（如：“写”这个字由90%概率是动词，10%是名词）。\n",
    "\n",
    "**综合公式**：\n",
    "$$ P^*(W, T) = \\alpha \\times [\\text{子模型1}] + \\beta \\times [\\text{子模型2}] $$\n",
    "*   通过参数 $\\alpha$ 和 $\\beta$ 来调节权重。\n",
    "    *   如果数据稀疏（生僻词多），子模型1（基于词性）更可靠，因为词性转移规律很稳定。\n",
    "    *   如果常见词多，子模型2（基于词汇）更准确，因为它捕捉了具体的习惯用语。"
   ],
   "id": "4379d6b24cb962d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 待实现\n",
    "# 困难的\n",
    "# 有空再说"
   ],
   "id": "fc33179d8b996c34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# n元模型的问题\n",
    "\n",
    "该 课程 很 <span style=\"color:purple\">枯燥</span>，大家 觉得 很 <span style=\"color:purple\">无聊</span>。\n",
    "\n",
    "\n",
    "$$\n",
    "P(\\text{无聊}|\\text{很}) = \\frac{\\text{count(很 无聊)}}{\\text{count(很)}}\n",
    "$$\n",
    "\n",
    "\n",
    "## 问题①：数据稀疏\n",
    "N-元组“很 无聊”未出现过，则回退\n",
    "\n",
    "\n",
    "## 问题②：忽略语义相似性\n",
    "“无聊”与“枯燥”虽语义相似，但无法共享信息\n",
    "\n",
    "## 解决方案：基于连续语义空间的词语表示\n",
    "\n",
    "简单来说就是，原来是采用独热编码，即在模型眼里：\n",
    "\n",
    "$$向量(枯燥)×向量(无聊)=0$$\n",
    "\n",
    "但这俩是近义词，因此需要用稠密的实数向量代替\n",
    "\n",
    "通过模型学习，可以把语义相近的放在相近的空间，使用时会表现出和近义词相似的规律"
   ],
   "id": "6a9bb6b41602c1a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 语言模型的发展\n",
    "\n",
    "+ 阶段一（约1970之前），基于规则的语言模型：主要基于手写规则。\n",
    "+ 阶段二（约1970-约2000），基于统计的语言模型：从数学统计的角度预测下个词的出现概率，如N-Gram等，推理过程非常直观，但是推理结果非常受数据集的影响，容易出现数据稀疏（即空值）等问题。\n",
    "+ 阶段三（约2000-约2017），基于神经网络的语言模型：比如NNLM、RNN、LSTM等。\n",
    "+ 阶段四（约2018-现在），基于Transformer的语言模型：比如BERT、GPT系列等大模型。\n",
    "\n",
    "宗成庆:《自然语言处理》\n",
    "\n",
    "谢谢！"
   ],
   "id": "21af6e3a25d0d803"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
